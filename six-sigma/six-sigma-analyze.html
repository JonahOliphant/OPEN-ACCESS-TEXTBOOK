<!doctype html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Process and Project Management Open Access Textbook Chapter: Project Management Basics</title>
  <link href="../simpleGridTemplate.css" rel="stylesheet" type="text/css">
  <style>
    @import url('https://fonts.googleapis.com/css2?family=Barlow&display=swap');
  </style>
<style>
  @import url('https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap');
  </style>
</head>

<body>
  <!-- Main Container -->
  <div class="container">
    <!-- Header -->
    <header class="header">
      <nav>
        <ul>
          <li><a href="/index.html">Overview</a></li>
          <li><a href="/project-management/index.html">1. Project Management</a></li>
          <li><a href="/process-mgmt/index.html">2. Process Management</a></li>
          <li><a href="/six-sigma/index.html" id="current">3. Six Sigma</a></li>
          <li><a href="/lean/index.html">4. Lean Management</a></li>
          <li><a href="/agile/index.html">5. Agile Management</a></li>
          <li><a href="/tying-together/index.html">6. Tying it All Together</a></li>
          <li><a href="/futuredirections.html">7. Future Directions</a></li>
        </ul>
      </nav>
    </header>
    <div class="sidebar">
      <button class="m">↹</button>
      <h6><a href="/index.html">Overview</a></h6>
      <h6><a href="/project-management/index.html">Project Management</a></h6>
      <h6><a href="/process-mgmt/index.html">Process Managment</a></h6>
      <h6><a href="/six-sigma/index.html" id="current">Six Sigma</a></h6>
      <h2><a href="/six-sigma/six-sigma.html">Six Sigma Overview</a></h2>
      <h2><a href="/six-sigma/six-sigma-define.html">Six Sigma Define Phase</a></h2>
      <h2><a href="/six-sigma/six-sigma-measure.html">Six Sigma Measure Phase</a></h2>
      <h2><a href="/six-sigma/six-sigma-analyze.html" id="current">Six Sigma Analyze Phase</a></h2>
      <h2><a href="/six-sigma/six-sigma-improve.html">Six Sigma Improve Phase</a></h2>
      <h2><a href="/six-sigma/six-sigma-control.html">Six Sigma Control Phase</a></h2>
      <h2><a href="/six-sigma/design-for-six-sigma.html">Design for Six Sigma</a></h2>
      <h6><a href="/lean/index.html">Lean Managment</a></h6>
      <h6><a href="/agile/index.html">Agile Management</a></h6>
      <h6><a href="/tying-together/index.html">Tying it All Together</a></h6>
      <h6><a href="/futuredirections.html">Future Directions</a></h6>
    </div>
    </header>
    <!-- Hero Section -->
    <section class="intro2">
      <h2 class="white"><a class="white" href="index.html">Six Sigma Process Management</a>: Analyze Phase</h2>
      <img class="pic" src="../images/SIX SIGMA.jpg" alt="Six Sigma (6&sigma;) Process Management" />
    </section>
    <!-- Stats Gallery Section -->
    <div class="gallery">
      <h1>Analyze Phase</h1>
      <p>The main activity in the Analyze phase is to identify the potential root cause of the problem and arrive at the
        actual root cause. Six sigma teams use various statistical tools in the Analyze phase to identify the root
        cause(s). </p>
      <p>As noted by Mundro, Ramu &amp; Zrymiak (2015), this phase determines how well or how poorly the process is
        currently performing and identifies possible root causes for variation in quality. The data analyzed can reveal
        the basic nature and behavior of the process, and show how capable and stable the process is over an extended
        period of time. The analyze phase covers two major sections. The first part covers exploratory data analysis,
        which includes items such as multivariate studies to differentiate variation and simple linear correlation and
        regression to determine the statistical significance and difference between correlation and causation. The
        second part offers an introduction to hypothesis testing to determine statistical significance, which includes
        items such as tests for means, variances, and proportions, paired-comparison hypothesis tests, and analysis of
        variance (ANOVA).  </p>
      <p>Some processses that are commonly analyzed include (but are not limited to): </p>
      <ul>
        <li>Billing a customer</li>
        <li>Developing new products</li>
        <li>Processing customer orders</li>
        <li>Upgrading software</li>
        <li>Managing payroll</li>
        <li>Hiring employees</li>
        <li>Budgeting</li>
        <li>Paying bills</li>
        <li>Evaluating vendors</li>
        <li>Improving distribution of products</li>
        <li>Managing inventory</li>
      </ul>
      <h2>Root Cause Analysis</h2>
      <p>A root cause is defined as a factor that caused a nonconformance and should be permanently eliminated through
        process improvement. The root cause is the core issue—the highest-level cause—that sets in motion the entire
        cause-and-effect reaction that ultimately leads to the problem(s). (American Society for Quality, n.d.) Root
        cause analysis (RCA), therefore, is a systematic process for identifying root causes of events (often seen as
        problems) and an approach for responding to them. The purpose is to pinpoint factors that contribute to an event
        or problem. This is accomplished by identifying what happened, how it happened, why it happened, and actions
        that can be taken to prevent future recurrance. It is important to bear in mind that in most cases there are
        multiple contributing factors to a problem or event, but some factors have more impact than others. </p>
      <p>There are a variety of analysis methods that can be used for root cause analysis although in each case the goal
        is to identify all and multiple contributing factors to a problem or event. This is most effectively
        accomplished through an analysis method. Some methods used in RCA include:</p>
      <blockquote>
        <h3>Five Whys Analysis</h3>
        <p>The five whys is a very simple problem solving technique that helps get to the root causes of a probelm
          quickly. You identify an issue and then to get to its root cause you ask yourself &quot;why did this
          occur&quot;. Then, ask yourself &quot;why did this occur&quot; for your answer. This continues for a series of
          times until a root cause or causes becomes clear. This methodology was used in the 1970s by Toyota motor
          company. </p>
        <h3>Pareto chart Analysis </h3>
        <p><a href="/project-management/quality-mgmt.html" class="black">Project Management Quality
            Management</a> discusses Pareto charts, but they are also used in this phase. </p>
        <h3>Barrier Analysis</h3>
        <p>Barrier analysis involvels tracing pathways by which an intended outcome is adversely affected by a hazard or
          issue, including the identification of any failed or missing countermeasures that could or should have
          prevented the undesired effect(s). Basically, it is finding and mitigrating barriers to intended outcomes.
        </p>
        <h3>Causal Factor Tree Analysis</h3>
        <p>Casual factor tree analysis is an analysis technique used to record and display, in a logical,
          tree-structured hierarchy, all the actions and conditions that were necessary and sufficient for a given
          consequence to have occurred.</p>
        <h3>Fishbone Diagram (Ishikawa Diagram)</h3>
        <p>Fishbone Diagrams are used in this phase but are more commonly associated with the <a
            href="six-sigma-improve.html" class="black">Six Sigma Improve phase</a>, so are discussed in detail there.
        </p>
        <h3>Failure Mode and Effects Analysis (Fmea)</h3>
        <p>Failure Mode and Effects Analysis is used in this phase but are more commonly associated with the <a
            href="six-sigma-improve.html" class="black">Six Sigma Improve phase</a>, so are discussed in detail
          there.<br>
        </p>
      </blockquote>
      <h2>Inference and Prediction</h2>
      <p><strong>Prediction</strong> is foretelling a future event or an occurrence. <strong>Inference</strong> is a
        similar concept, but the theory about teh future event or occurrence is inferred by analyzing the evidence,
        facts, and clues. There are many times in business when assumptions about what may occur in the future occur so
        that decisions can be made, from what customer purchasing trends may be to the availability of supplier
        resources. The idea is to do all that is possible to be correct. One thing that may be helpful is to consider
        ranges of results instead of a single result. Another thing that may be helpful is to make iteritive predictions
        -- smaller but more frequent predictions upon which you can adjust business practices as the situation evolves.
      </p>
      <h2>Experiment</h2>
      <p>As summarized by Scott Stevens James Madison University, &quot;An <strong>experiment</strong> is any
        well-defined, repeatable procedure, usually involving one or more chance events. One repetition of the procedure
        is called a . When a trial is conducted, it results in some <strong>outcome</strong>. (Note that, in the usual
        case where the experiment involves randomness, different trials can result in different outcomes.) A
        <strong>random variable</strong> is a measurable (numeric) quantity associated with the outcome of an
        experiment. An <strong>event</strong> is a statement about the outcome of the experiment that is either true or
        false.&quot; </p>
      <h2>Hypothesis Testing</h2>
      <p>In order to determine root causes, we may come up with hypothesis as to what is going amiss.  Hypothesis
        testing involves making an initial assumption, collecting evidence (data) and then, based on the available
        evidence (data), deciding whether to reject or not reject the initial assumption. When making the initial
        assumption, a null hypoethesis and alternative hypothesis are created. An <strong>alternative hypothesis
        </strong>is the hypothesis that we are interested in proving. The<strong> null hypothesis </strong>is the
        complement of the alternative hypothesis. It is the hypothesis that there is no significant difference between
        specified samples/populations, any observed difference is instead due to sampling or experimental error. This
        may be best explained with an example. Let's say I hypothesize that perhaps if a person were to add one teaspoon
        of lemon juice to a full load of colored laundry then the laundry would come out cleaner. The null hypothesis
        would say that adding the lemon juice made no diffference. The alternative hypothesis would say that it did.
        With hypoethesis testing, we need to consider possible type I and type II errors. In <strong>type I
          error</strong> (also known as false positive) one rejects the null hypothesis when it is correct (in my
        example above the result says lemon juice makes a difference when it actually does not). In <strong>type II
          error </strong>(also known as false negative) one accepts the null hypothesis when it is not correct (in my
        example above the result says lemon juice does not make a difference when it actually does). </p>
      <p>Below are the basic steps for hypothesis testing: </p>
      <ul>
        <li>Step 1: State the null hypothesis</li>
        <li>Step 2: State the alternative hypothesis.</li>
        <li>Step 3: Set up the test/experiment</li>
        <li>Step 4: Collect data</li>
        <li>Step 5: Calculate test statistics</li>
        <li>Step 6: Assess for acceptance or rejection</li>
        <li>Step 7: Draw conclusion </li>
      </ul>
      <h2>The impact of variability </h2>
      <p>In Six Sigma process management, one of the most significant goals is to minimize varation. As noted by Tom
        Mitchell of University of Baltimore (n.d.), "Our goal is to minimize variation within all of our critical
        processes. Quantitatively, this means working towards Six Sigma quality, or fewer than 3.4 defects per million
        &ldquo;opportunities.&rdquo; An opportunity is defined as a chance for non-conformance or not meeting required
        specifications. Culturally, this means needing to learn how to be nearly flawless in executing key processes
        because flawless execution is critical to both goals – customer satisfaction and increased productivity." He
        then wisely notes, "customers feel the variance, not the mean." It is important to note that in some cases
        special cause variation can build a positive impact into a process or product, but in most cases it is something
        we would rather eliminate or minimize.</p>
      <p>When a process is stable and in control, it displays common cause variation (variation that is inherent to the
        process). A process is <strong>stable and in control </strong>when it can be predicted how the process will vary
        (within limits) in the future based on prior experience. If the process is <strong>unstable</strong>, the
        process displays special cause variation, non-random variation from external factors.</p>
      <blockquote>
        <h3>Common Cause Variation</h3>
        <p>Common cause variation is inherrent in a process; it is random variation present in stable processes.</p>
        <h3>Special Cause Variation </h3>
        <p>Speial cause variation is non-random variation from external factors; it is an unpredictable deviation
          resulting from a cause that is not an intrinsic part of a process.</p>
        <h3>Controlled variation</h3>
        <p>Controlled variation<em>&nbsp;</em>is characterized by a stable and consistent pattern of variation over
          time, and is associated with common causes. A process operating with controlled variation has an outcome that
          is predictable within the bounds of the control limits.</p>
        <h3>Uncontrolled Variation</h3>
        <p>Uncontrolled variation is characterized by variation that changes over time and is associated with special
          causes. The outcomes of this process are unpredictable; a customer may be satisfied or unsatisfied given this
          unpredictability.</p>
      </blockquote>
      <p>A statistical term that is used to describe variability is variance. <strong>Variance</strong> is the
        measurement of the spread about the mean. It is calculated by taking the square of the differences between each
        data value and the mean, and then dividing by one less than the number of concentrations. It is the square of
        the standard deviation.</p>
      <h3>Variability Factors</h3>
      <p dir="ltr">The factors that cause variability in product functions are called error factors or noise.</p>
      <ul>
        <li>Factors due to environmental conditions – outer noise (ex: variations in raw materials; mental state of
          operators)</li>
        <li>Factors due to deterioration – inner noise&nbsp;</li>
        <li>Factors due to variation among products – inner noise (humidity, dust, vibrations)</li>
      </ul>
      <p>To study each factor independently,&nbsp; the plan consists of two orthogonal arrays – the inner array and the
        outer array. The inner array consists of control factors, and the outer array consists of noise and signal
        factors.</p>
      <h2>statistics and Data Analysis distributions</h2>
      <p><strong>Statistics</strong> is a  branch of mathematics dealing with the collection, analysis, interpretation,
        and presentation of masses of numerical data. There are five basic steps to statistics and the subsequent data
        analysis: 1. Collect data; 2. Organize the data; 3. Analyze the data; 4. Interpret the data; and 5. Present the
        data. As data analysis is performed, the data will be distributed in some type of pattern. A few common types of
        distributions are normal and binomial. </p>
      <blockquote>
        <h3>Normal distribution</h3>
        <p>With a normal distribution the data is distributed around a central value with no bias left or right. This is
          commonly known as a <strong>bell curve </strong>and looks like this:</p>
        <p><a title="Ark0n, CC BY-SA 3.0 &lt;http://creativecommons.org/licenses/by-sa/3.0/&gt;, via Wikimedia Commons"
            href="https://commons.wikimedia.org/wiki/File:Iqr.png"><img width="512" alt="Iqr"
              src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Iqr.png/512px-Iqr.png"></a>
        </p>
        <h3>Binomial distribution</h3>
        <p>A binomial distribution can be thought of as simply the probability of one of two outcomes (success/failure,
          yes/no, etc) in an experiment or survey that is repeated multiple times. The prefix “bi” means two, or twice.
        </p>
      </blockquote>
      <p>In many cases, a focus of statistics is understanding how far results may deviate from the mean. This is known
        as standard deviation. <strong>Standard Deviation (Std Dev)</strong> is the easurement of the spread about the
        mean. It is the square root of the differences between each data value and the mean, divided by one less than
        the number of concentrations. It is the square root of the variance. </p>
      <h2>probability Distributions and Central Limit Theorem </h2>
      <p>A <strong>probability distribution</strong> ("sampling distribution")  is a statistical function that describes
        all the possible values and likelihoods that a random variable can take within a given
        range.The <strong>range</strong> is the difference between the lowest and highest values in a distribution. It
        is a commonly used measure of variability. </p>
      <p>A key part of statistics is the <strong>central limit theorum</strong>, which states that the distribution of
        sample means approximates a normal distribution as the sample size gets larger. Based on this theorum, the means
        of a random sample of size, <em>n</em>, from a population with mean, µ, and variance, σ<sup>2</sup>, distribute
        normally with mean, µ, and variance,  σ<sup>2</sup>/</span></span></span></span> <span
          id="MathJax-Span-12"><span id="MathJax-Span-13">n</span></span></span></span></span></span></span></span> as
        the sample size increases. It states that the sampling distribution of means is normally distributed even for
        non-normal raw data distributions (such as data that is skewed one direction or the other) with larger samples.
      </p>
      <p><a
          title="Daniel Resende, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons"
          href="https://commons.wikimedia.org/wiki/File:Central_Limit_Theorem.png"><img width="512"
            alt="Central Limit Theorem"
            src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Central_Limit_Theorem.png/512px-Central_Limit_Theorem.png"></a>
      </p>
      <h3>Confidence Intervals </h3>
      <p>A <strong>confidence interval </strong>is a range of values that describes the uncertainty surrounding an
        estimate. A confidence interval is indicated by its endpoints. It is actually an estimate in itself.
        <strong>Confidence limits</strong> are the numbers at the upper and lower end of a confidence interval. The
        confidence limits of a measurement are the limits between which the measurement error is with a probability P.
        Most people use 90%, 95%, or 99% confidence limits, although you could use other values. The <strong>confidence
          level </strong>(typically 90 to 99%) reports a range of concentrations within which a particular parameter
        would likely occur if samples were taken repeatedly from the sample distribution. </p>
      <h3>Correlation versus Causation </h3>
      <p><strong>Correlation</strong> is a term in statistics that refers to the degree of association between two
        random variables (or tests for a relationship between two variables). If two or more variables are correlated,
        their directional movements are related. If two variables are <strong>positively</strong> correlated, it means
        that as one goes up or down, so does the other. Alternatively, if two variables are <strong>negatively</strong>
        correlated, one goes up while the other goes down. <strong>No correlation </strong>is when two variables are
        completely unrelated and a change in A leads to no changes in B, or vice versa. </p>
      <p>A correlation’s strength can be quantified by calculating the <strong>correlation coefficient,</strong>
        sometimes represented by r. The correlation coefficient falls between negative one and positive one.</p>
      <ul>
        <li>r = -1 indicates a perfect negative correlation.</li>
        <li>r = 1 indicates a perfect positive correlation.</li>
        <li>r = 0 indicates no correlation.</li>
      </ul>
      <p><strong>Causation</strong> (also known as causality) means that one variable caused the other to occur. Proving
        a causal relationship between variables requires a true experiment with a control group (which doesn’t receive
        the independent variable) and an experimental group (which receives the independent variable). It means A and B
        have a cause-and-effect relationship with one another where A causes B. However, seeing two variables moving
        together does not necessarily mean we know whether one variable <em>causes</em> the other to occur. A strong
        correlation could indicate causality but there could easily be other explanations. For example, it could be due
        to random chance or another variable that makes the relationship seem stronger than it is. To see some examples,
        visit the <a href="https://www.tylervigen.com/spurious-correlations" target="_blank" class="black">spurious
          correlations</a> website. To test your data for causation one would use design of experiments. <br>
      </p>
      <h2>design of experiments</h2>
      <p><strong>Design of experiments</strong> is used to test data for causation and are built for efficiency. It is a
        series of tests in which purposeful changes are made to the input variables of a system or process and the
        effects on response variables are measured. This might include randomized and experimental study,
        quasi-experimental study, correlational study, or singlestudy. In<strong> experimental design,</strong> there is
        a control group and an experimental group, both with identical conditions but with one independent variable
        being tested. <strong>Quasi-experimental </strong> <strong>studies</strong> typically require more advanced
        statistical procedures to get the necessary insight and occur when you can&rsquo;t randomize the process of
        selecting users to take the study. A <strong>correlational study</strong> is when you try to determine whether
        two variables are correlated or not. <strong>Single study</strong> (also known as single subject research or
        single case experiments) has a participant that serves as both the control and treatment grouip. This research
        design is useful when the researcher is attempting to change the behavior of an individual or a small group of
        individuals and wishes to document that change. </p>
      <p>Tellford (2009) identified the fundamental principles of design of experiments as randomization, replication,
        blocking, orthogonality, and factorial experimentation. Each are described below. </p>
      <blockquote>
        <p><strong>Randomization</strong>. A method that protects against an unknown bias distorting the results of the
          experiment by testing the sequence of baseline and other measures in random order so unknown bias can average
          out.</p>
        <p><strong>Replication</strong>. A method that increases the sample size and is a method for increasing the
          precision of the experiment. A replicate is a complete repetition of the same experimental conditions,
          beginning with the initial setup.</p>
        <p><strong>Blocking</strong>. A method for increasing precision by removing the effect of known nuisance
          factors. In a blocked design, both the baseline and new procedures are applied to samples of material from one
          batch, then to samples from another batch, and so on. </p>
        <p><strong>Orthogonality</strong>, The factors in an orthogonal experiment design are varied independently of
          each other. The main results of data collected using this design can often<br>
          be summarized by taking differences of averages and can be shown graphically by using simple plots of suitably
          chosen sets of averages.</p>
        <p><strong>Factorial experimentation.</strong> A method in which the effects due to each factor and to
          combinations of factors are estimated. Factorial designs investigate the effects of many different factors by
          varying them simultaneously instead of changing only one factor at a time. Factorial designs allow estimation
          of the sensitivity to each factor and also to the combined effect of two or more factors. </p>
      </blockquote>
      <p>A full introduction can be found at the Johns Hopkins University Applied Physics Laboratory article <a
          href="https://www.jhuapl.edu/Content/techdigest/pdf/V27-N03/27-03-Telford.pdf" target="_blank" class="black">A
          Brief Introduction to Design of Experiments</a> by Jacqueline K. Telford </p>
      <h2>Regression analysis </h2>
      <p><strong>Regression analysis </strong>is a method of identifying which variables have impact on a topic of
        interest by mathematically sorting out variables that have an impact from many possible variables. Let's say you
        are a company that sells corn dogs from a food truck. You know there are many factors that can impact your
        sales. In simply looking at weather you know that if it is too cold or hot or rainy or snowy or windy it will
        have an impact. You also know that holidays and school schedules also have a sales impact; particularly on what
        individuals elect to purchase. It also seems that a large impact is where your truck is located for the day. You
        feel this may perhaps be the most important factor influencing sales. Regression analysis is a mathematical way
        of determining which variables do indeed have an impact and expresses which factors matter the most. It also
        addresses how different factors interact with one another and, importantly, tells us the level of certainty.</p>
      <p>To perform a regression analysis, gather data on the variables in question. In our case lets consider how windy
        it has been in the past year (independent variable) and compare it to the daily sales numbers (dependent
        variable). </p>
      <p>Next we plot all of our information in a scatter diagram with the y-axis being the dependent variable and the
        x-axis being the independent variable. Ours might look something like this:</p>
      <p><img src="../images/six-sigma/scatter-diagram-windspeed.png" width="500" height="300" alt="" /></p>
      <p>It does seem to show that lower wind speeds tend toward higher daily sales, but by how much? To find out, next
        we add a regression line (linear trendline) that shows roughly the middle of the data points:</p>
      <p><img src="../images/six-sigma/scatter-diagram-windspeed-w-trendline.png" width="479" height="287" alt="" /></p>
      <p>This shows the line that best fits the data and demonstrates that there is indeed a negative correlation
        between the average wind speed and the daley sales in hundreds of dollars.</p>
      <p>If you use a statistics program then it also outputs a formula that explains the slope of the line.</p>
      <p><img src="../images/six-sigma/scatter-diagram-windspeed-trend-detail.png" width="500" height="300" alt="" />
      </p>
      <p>Our information shows <br>
        y = -0.7269x + 15.942<br>
        R² = 0.3404 </p>
      <p>What this formula is telling us is that if there is no <em>x</em> then <em>y</em> = 15.942. So, historically,
        when there was no wind at all we vmade an average of 15.942 hundred dollars ($1,594.20) in sales and you can
        expect to do the same going forward assuming other variables stay the same. And in the past, for every
        additional mile per hour of wind, we made an average of .7269 hundred dollars ($72.69) in sales. </p>
      <p>The <strong>R-Squared </strong>(R² or the coefficient of determination) is a statistical measure in a
        regression model that determines the proportion of variance in the dependent variable that can be explained by
        the independent variable. In other words, r-squared shows how well the data fit the regression model (the
        goodness of fit). R-squared values range from 0 to 1 (and are commonly stated as percentages from 0% to 100%).
        An R-squared of 100% means that all movements of a dependent variable is completely explained. In our case only
        34.04% of the movement is explained. This means it basically had only a small effect. </p>
      <p>A good review article can be found in a Harvard Busines Review written by Amy Gallow (2015) called <a
          href="https://hbr.org/2015/11/a-refresher-on-regression-analysis" target="_blank" class="black">A Refresher on
          Regression Analysis</a>.<br>
      </p>
      <h2>References</h2>
      <p>American Society for Quality. (n.d.) What is Root Cause Analysis. Retrieved July 14, 2021 from <a
          href="https://asq.org/quality-resources/root-cause-analysis"
          class="black">https://asq.org/quality-resources/root-cause-analysis</a> </p>
      <p>Munro, R., Ramu, G., &amp; Zrymiak, D. (2015).  The Certified Six Sigma Green Belt Handbook.  Second edition.
      </p>

      <p>Stevens, S. (n.d.). Probability and Expected Value. Retrieved June 28, 2022 from <a
          href="https://www.fulsheartexas.gov/docs/CC%20Minutes/Probability_and_expected_value.doc" target="_blank"
          class="black">https://www.fulsheartexas.gov/docs/CC%20Minutes/Probability_and_expected_value.doc</a></p>
      <p>Tellford, J. K. (2007). A Brief Introduction to Design of Experiments. Johns Hopkins Technical Digest. 27(3).
        Retrieved June 29, 2022 from <a href="https://www.jhuapl.edu/Content/techdigest/pdf/V27-N03/27-03-Telford.pdf"
          class="black">https://www.jhuapl.edu/Content/techdigest/pdf/V27-N03/27-03-Telford.pdf</a></p>
      <p>&nbsp;</p>
    </div>
    <!-- Footer Section -->
    <!-- Copyrights Section -->
    <div class="copyright">&copy;2021 - <a href="https://www.linkedin.com/in/annearendt/" class="back">Dr. Anne
        Arendt</a></div>
    <!-- Main Container Ends -->
    <script type="text/javascript" src="/js/main.js"></script>
</body>

</html>